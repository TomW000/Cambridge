{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0924a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb2c419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d004fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, embeddings_path, curated_idx, few_shot_transforms, model\n",
    "from setup import tqdm, torch, np, os, plt, tqdm\n",
    "from analysis_utils import display_hdf_image_grid, resize_hdf_image, get_augmented_coordinates\n",
    "from setup import cosine_similarity, euclidean_distances\n",
    "from perso_utils import get_fnames, load_image, get_latents\n",
    "from DINOSim import DinoSim_pipeline\n",
    "from napari_dinosim.utils import get_img_processing_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fba314",
   "metadata": {},
   "source": [
    "### Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258e6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = DinoSim_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n",
    "\n",
    "files, labels = zip(*get_fnames()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c8d46",
   "metadata": {},
   "source": [
    "### Resize coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c3da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = resize_size/130\n",
    "resize = lambda x: resize_factor*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bc90d",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Extracting reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a8b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['A','D','Ga','Glu','O','S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39d302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "    [1,3,6,8,9],\n",
    "    [1,2,4,5,6],\n",
    "    [0,1,2,4,5],\n",
    "    [1,2,3,6,7],\n",
    "    [1,2,5,6,8],\n",
    "    [0,6,10,11,14]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d89351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [\n",
    "    [(69,63.5),(68,61),(83,57),(76,62),(60,63)],\n",
    "    [(66,62),(58.5,64),(64,60),(62.5,65),(64,71)],\n",
    "    [(65,67),(72,60),(63,72),(60,67),(69,66.5)],\n",
    "    [(65,66),(64,71),(62,58.5),(62,68),(69,55)],\n",
    "    [(66,60),(60,70),(61,66.6),(58.5,63.5),(62.5,70.5)],\n",
    "    [(63,73),(58,69),(60,69),(66,64),(62,71)]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab31f83",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7e429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=False):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path, weights_only=False)\n",
    "        return mean_ref\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "            ref_embs_list = []\n",
    "            for i, index in tqdm(enumerate(indices)):\n",
    "                dataset_slice = files[i*600:(i+1)*600]\n",
    "                imgs = [resize_hdf_image(load_image(dataset_slice[k])[0]) for k in index]\n",
    "                coordinates = [list(map(resize, c)) for c in coords[i]]\n",
    "                dataset = list(zip(imgs, coordinates))\n",
    "                class_wise_embs_list = []\n",
    "                for image, reference in dataset:\n",
    "                    few_shot.pre_compute_embeddings(\n",
    "                        image[None,:,:,:],\n",
    "                        verbose=False,\n",
    "                        batch_size=1\n",
    "                    )\n",
    "                    few_shot.set_reference_vector(get_augmented_coordinates(reference), filter=None)\n",
    "                    closest_embds = few_shot.get_k_closest_elements(k=k, return_indices=False) # list of vectors\n",
    "                    class_wise_embs_list.append(torch.mean(closest_embds.cpu(), dim=0)) # list of lists of vectors\n",
    "                ref_embs_list.append(class_wise_embs_list) # list of lists of lists of vectors\n",
    "\n",
    "            ref_embs = np.array([np.mean(class_closest_embs, axis=0) for class_closest_embs in ref_embs_list])\n",
    "            \n",
    "            torch.save(ref_embs, os.path.join(embeddings_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "            \n",
    "            return ref_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4094ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_refs = compute_ref_embeddings(True, '/home/tomwelch/Cambridge/Embeddings/giant_mean_ref_518_Aug=False_k=10')\n",
    "#mean_refs = compute_ref_embeddings(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c753b8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef compute_ref_embeddings(saved_ref_embeddings=False, \\n                           embs_path=None, \\n                           k=10,\\n                           data_aug=True):\\n\\n    if saved_ref_embeddings:\\n        \\n        mean_ref = torch.load(embs_path)\\n\\n    else:\\n\\n        if data_aug:    \\n            nb_transformations = len(few_shot_transforms)\\n            \\n            # Preload images and metadata once\\n            good_images = []\\n            transformed_coordinates = []\\n\\n            for idx in curated_idx:\\n                img, coord_x, coord_y = load_image(files[idx])\\n                good_images.append(img.transpose(1,2,0))\\n                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\\n\\n            transformed_images = []\\n            for image in good_images:\\n                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\\n                transformed_images.extend(transformed)\\n\\n            for j, img in enumerate(transformed_images):\\n                if img.shape != torch.Size([130, 130, 1]):\\n                    h, w = img.shape[:2]\\n                    h_diff = (130 - h) // 2\\n                    w_diff = (130 - w) // 2\\n                    padded_img = torch.zeros(130, 130, 1)\\n                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\\n                    transformed_images[j] = padded_img\\n                    \\n            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\\n            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\\n            good_datasets = np.array(good_datasets)\\n            \\n            transformed_coordinates = np.vstack(transformed_coordinates)\\n            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\\n\\n        else:\\n\\n            imgs_coords = [load_image(files[idx]) for idx in curated_idx]\\n            imgs, xs, ys = zip(*imgs_coords)\\n\\n            batch_size = int(len(curated_idx)/len(neurotransmitters))\\n            imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\\n            good_datasets = np.array(imgs).transpose(0,1,3,4,2)\\n            \\n            good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\\n            good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\\n            good_coordinates = np.array(good_coordinates)\\n\\n\\n        unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\\n        for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\\n            \\n            # Pre-compute embeddings\\n            few_shot.pre_compute_embeddings(\\n                dataset,  # Pass numpy array of images\\n                overlap=(0.5, 0.5),\\n                padding=(0, 0),\\n                crop_shape=(518, 518, 1),\\n                verbose=True,\\n                batch_size=10\\n            )\\n            \\n            # Set reference vectors\\n            few_shot.set_reference_vector(coordinates, filter=None)\\n            ref = few_shot.get_refs()\\n            \\n            # Get closest elements - using the correct method name\\n            close_embedding =  few_shot.get_k_closest_elements(k=k)\\n            k_labels =  [batch_label for _ in range(k)]\\n\\n            \\n            # Convert to numpy for storing\\n            close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\\n            \\n            filtered_latent_list.append(close_embedding_np)\\n            filtered_label_list.append(k_labels)\\n            \\n            # Clean up to free memory\\n            few_shot.delete_precomputed_embeddings()\\n            few_shot.delete_references()\\n\\n        mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\\n        # Stack all embeddings and labels\\n        ref_latents = np.vstack(filtered_latent_list)\\n        ref_labels = np.hstack(filtered_label_list)\\n        \\n        torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\\n        torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\\n        torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=True):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "            imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "            imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "            good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "            \n",
    "            good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "            good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "            good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "        unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "        for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "            \n",
    "            # Pre-compute embeddings\n",
    "            few_shot.pre_compute_embeddings(\n",
    "                dataset,  # Pass numpy array of images\n",
    "                overlap=(0.5, 0.5),\n",
    "                padding=(0, 0),\n",
    "                crop_shape=(518, 518, 1),\n",
    "                verbose=True,\n",
    "                batch_size=10\n",
    "            )\n",
    "            \n",
    "            # Set reference vectors\n",
    "            few_shot.set_reference_vector(coordinates, filter=None)\n",
    "            ref = few_shot.get_refs()\n",
    "            \n",
    "            # Get closest elements - using the correct method name\n",
    "            close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "            k_labels =  [batch_label for _ in range(k)]\n",
    "\n",
    "            \n",
    "            # Convert to numpy for storing\n",
    "            close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "            \n",
    "            filtered_latent_list.append(close_embedding_np)\n",
    "            filtered_label_list.append(k_labels)\n",
    "            \n",
    "            # Clean up to free memory\n",
    "            few_shot.delete_precomputed_embeddings()\n",
    "            few_shot.delete_references()\n",
    "\n",
    "        mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\n",
    "        # Stack all embeddings and labels\n",
    "        ref_latents = np.vstack(filtered_latent_list)\n",
    "        ref_labels = np.hstack(filtered_label_list)\n",
    "        \n",
    "        torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7274",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_neurotransmitters = np.zeros((len(neurotransmitters),len(neurotransmitters))) + np.identity(len(neurotransmitters))\n",
    "emb_labels = np.hstack([[neuro]*feat_dim*600 for neuro in neurotransmitters]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859a151",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Datasetwide Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480883e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/3600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 3600/3600 [00:52<00:00, 69.05it/s]\n"
     ]
    }
   ],
   "source": [
    "files, _ = zip(*get_fnames()) \n",
    "images = np.array([resize_hdf_image(load_image(file)[0]) for file in tqdm(files, desc='Loading images')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af93d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947ec113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing embeddings\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(saved_embeddings=False,\n",
    "                       embs_path=None):\n",
    "\n",
    "        if saved_embeddings:\n",
    "                new_embeddings = torch.load(embs_path)\n",
    "\n",
    "        else:\n",
    "                #files, _ = zip(*get_fnames()) \n",
    "                #images = np.array([resize_hdf_image(load_image(file)[0]) for file in files])\n",
    "                for i in range(0,3600, 50):\n",
    "                        batch = images[i:i+50]\n",
    "                        few_shot.pre_compute_embeddings(\n",
    "                                batch,\n",
    "                                batch_size=1, \n",
    "                                verbose=False\n",
    "                                )\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                        del batch\n",
    "                        yield few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "g = compute_embeddings()\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "length = 3600//50\n",
    "for _ in tqdm(range(length)): embeddings_list.extend(next(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef9b0c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute class-wise accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(reference_embeddings = mean_refs, \n",
    "                       embeddings = embeddings_list,\n",
    "                       metric = euclidean_distances,\n",
    "                       model_size = model_size,\n",
    "                       distance_threshold = 0.1,\n",
    "                       data_aug = True\n",
    "                       ):\n",
    "\n",
    "    score_lists = [[],[],[],[],[],[]]\n",
    "\n",
    "    #embeddings = embeddings.reshape(-1, feat_dim)\n",
    "    similarity_matrix = metric(reference_embeddings, embeddings)\n",
    "    similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "\n",
    "    similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= distance_threshold, similarity_matrix_normalized, 0)\n",
    "\n",
    "    for i, label in tqdm(enumerate(emb_labels)):\n",
    "\n",
    "        column = similarity_matrix_normalized_filtered[:,i]\n",
    "        j=0\n",
    "        if sum(column) == 0:\n",
    "            j+=1\n",
    "        else:\n",
    "            patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "            output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "            gt_index = neurotransmitters.index(label)\n",
    "            ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "            score = np.sum(output_class*ground_truth)\n",
    "            score_lists[gt_index].append(score)\n",
    "\n",
    "    accuracies = [np.mean(scores)*100 for scores in score_lists]\n",
    "    print(f'{j} embeddings did not pass the threshold')\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1645a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = compute_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6871dd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7), dpi=300)\n",
    "plt.bar(neurotransmitters, accuracies)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Mean hard accuracy')\n",
    "plt.title(f'Mean hard accuracies across classes - {model_size} DINOv2 - 140x140 images - Threshold = {distance_threshold} - Data augmentation: {data_aug}')\n",
    "plt.axhline(np.mean(accuracies), color='r', linestyle='--', label='Average')\n",
    "plt.axhline(y=(100/6), color='b', linestyle='--', label='Randomness')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,110])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
