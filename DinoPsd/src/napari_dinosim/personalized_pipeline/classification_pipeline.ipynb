{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0924a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d004fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, curated_idx, few_shot_transforms,  embeddings_path, model\n",
    "from setup import tqdm, torch, np, os, plt, tqdm, gc, sample\n",
    "from analysis_utils import display_hdf_image_grid, resize_hdf_image, get_augmented_coordinates\n",
    "from setup import cosine_similarity, euclidean_distances\n",
    "from perso_utils import get_fnames, load_image, get_latents\n",
    "from DINOSim import DinoSim_pipeline\n",
    "from napari_dinosim.utils import get_img_processing_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fba314",
   "metadata": {},
   "source": [
    "### Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = DinoSim_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n",
    "\n",
    "files, labels = zip(*get_fnames()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c8d46",
   "metadata": {},
   "source": [
    "### Resize coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = resize_size/130\n",
    "resize = lambda x: resize_factor*x\n",
    "\n",
    "from perso_utils import filter_f\n",
    "filter = filter_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bc90d",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Extracting reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['A','D','Ga','Glu','O','S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "    [1,3,6,8,9],\n",
    "    [1,2,4,5,6],\n",
    "    [0,1,2,4,5],\n",
    "    [1,2,3,6,7],\n",
    "    [1,2,5,6,8],\n",
    "    [0,6,10,11,14]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [\n",
    "    [(69,63.5),(68,61),(83,57),(76,62),(60,63)],\n",
    "    [(66,62),(58.5,64),(64,60),(62.5,65),(64,71)],\n",
    "    [(65,67),(72,60),(63,72),(60,67),(69,66.5)],\n",
    "    [(65,66),(64,71),(62,58.5),(62,68),(69,55)],\n",
    "    [(66,60),(60,70),(61,66.6),(58.5,63.5),(62.5,70.5)],\n",
    "    [(63,73),(58,69),(60,69),(66,64),(62,71)]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab31f83",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=False):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path, weights_only=False)\n",
    "        return mean_ref\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "            ref_embs_list = []\n",
    "            for i, index in tqdm(enumerate(indices)):\n",
    "                dataset_slice = files[i*600:(i+1)*600]\n",
    "                imgs = [resize_hdf_image(load_image(dataset_slice[k])[0]) for k in index]\n",
    "                coordinates = [list(map(resize, c)) for c in coords[i]]\n",
    "                dataset = list(zip(imgs, coordinates))\n",
    "                class_wise_embs_list = []\n",
    "                for image, reference in dataset:\n",
    "                    few_shot.pre_compute_embeddings(\n",
    "                        image[None,:,:,:],\n",
    "                        verbose=False,\n",
    "                        batch_size=1\n",
    "                    )\n",
    "                    few_shot.set_reference_vector(get_augmented_coordinates(reference), filter=None)\n",
    "                    closest_embds = few_shot.get_k_closest_elements(k=k, return_indices=False) # list of vectors\n",
    "                    class_wise_embs_list.append(torch.mean(closest_embds.cpu(), dim=0)) # list of lists of vectors\n",
    "                ref_embs_list.append(class_wise_embs_list) # list of lists of lists of vectors\n",
    "\n",
    "            ref_embs = np.array([np.mean(class_closest_embs, axis=0) for class_closest_embs in ref_embs_list])\n",
    "            \n",
    "            torch.save(ref_embs, os.path.join(embeddings_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "            \n",
    "            return ref_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_refs = compute_ref_embeddings(True, os.path.join(embeddings_path, 'giant_mean_ref_518_Aug=False_k=10'))\n",
    "#mean_refs = compute_ref_embeddings(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=True):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "            imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "            imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "            good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "            \n",
    "            good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "            good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "            good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "        unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "        for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "            \n",
    "            # Pre-compute embeddings\n",
    "            few_shot.pre_compute_embeddings(\n",
    "                dataset,  # Pass numpy array of images\n",
    "                overlap=(0.5, 0.5),\n",
    "                padding=(0, 0),\n",
    "                crop_shape=(518, 518, 1),\n",
    "                verbose=True,\n",
    "                batch_size=10\n",
    "            )\n",
    "            \n",
    "            # Set reference vectors\n",
    "            few_shot.set_reference_vector(coordinates, filter=None)\n",
    "            ref = few_shot.get_refs()\n",
    "            \n",
    "            # Get closest elements - using the correct method name\n",
    "            close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "            k_labels =  [batch_label for _ in range(k)]\n",
    "\n",
    "            \n",
    "            # Convert to numpy for storing\n",
    "            close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "            \n",
    "            filtered_latent_list.append(close_embedding_np)\n",
    "            filtered_label_list.append(k_labels)\n",
    "            \n",
    "            # Clean up to free memory\n",
    "            few_shot.delete_precomputed_embeddings()\n",
    "            few_shot.delete_references()\n",
    "\n",
    "        mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\n",
    "        # Stack all embeddings and labels\n",
    "        ref_latents = np.vstack(filtered_latent_list)\n",
    "        ref_labels = np.hstack(filtered_label_list)\n",
    "        \n",
    "        torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7274",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_neurotransmitters = np.eye(len(neurotransmitters))\n",
    "#emb_labels = np.hstack([[neuro]*int((resize_size/14)**2 * 600) for neuro in neurotransmitters]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859a151",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Datasetwide Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files, _ = zip(*get_fnames()) \n",
    "#images = np.array([resize_hdf_image(load_image(file)[0]) for file in tqdm(files, desc='Loading images')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ec113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_generator(batch_size=50): # TODO: if other batch_size, change in get_d_closed_elements!!!!!\n",
    "        \n",
    "        for i in range(0,len(images), batch_size):\n",
    "                batch = images[i:i+batch_size]\n",
    "                few_shot.pre_compute_embeddings(\n",
    "                        batch,\n",
    "                        batch_size=1, \n",
    "                        verbose=False\n",
    "                        )\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                del batch\n",
    "                yield few_shot.get_embeddings(reshape=False).cpu()\n",
    "                \n",
    "def compute_embeddings(batch_size=50):\n",
    "        g = embedding_generator()        \n",
    "\n",
    "        embeddings_list = []\n",
    "\n",
    "        length = len(images)//batch_size\n",
    "        for _ in tqdm(range(length)): embeddings_list.append(next(g))\n",
    "        new_embeddings = torch.cat(embeddings_list)\n",
    "        torch.save(embeddings_list, os.path.join(embeddings_path, f'{model_size}_dataset_embs_{resize_size}.pt'))\n",
    "        return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ecbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_embeddings = compute_embeddings()\n",
    "#new_embeddings = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt')) # takes ~ 45 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef9b0c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute class-wise accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39697",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from analysis_utils import get_threshold\n",
    "\n",
    "def compute_accuracies(reference_embeddings = mean_refs, \n",
    "                       embeddings = new_embeddings,\n",
    "                       metric = euclidean_distances,\n",
    "                       distance_threshold = 0.01\n",
    "                       ):\n",
    "\n",
    "    batch_size = int(len(embeddings)/6)\n",
    "\n",
    "    for n, i in tqdm(enumerate(range(0, len(embeddings), batch_size))):\n",
    "        batch = embeddings[i:i+batch_size]\n",
    "\n",
    "        #embeddings = embeddings.reshape(-1, feat_dim)\n",
    "        similarity_matrix = metric(reference_embeddings, batch)\n",
    "        similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "        threshold = get_threshold(similarity_matrix_normalized, 0.9)\n",
    "        similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= threshold, similarity_matrix_normalized, 0)\n",
    "\n",
    "        batch_score_list = []\n",
    "        for k in range(batch_size):\n",
    "\n",
    "            column = similarity_matrix_normalized_filtered[:,k]\n",
    "            j=0\n",
    "            if sum(column) == 0:\n",
    "                j+=1\n",
    "            else:\n",
    "                patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "                output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "                gt_index = n\n",
    "                ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "                score = np.sum(output_class*ground_truth)\n",
    "                batch_score_list.append(score)\n",
    "                \n",
    "        yield batch_score_list\n",
    "\n",
    "g = compute_accuracies()\n",
    "score_list = []\n",
    "\n",
    "for _ in range(6): score_list.append(next(g))\n",
    "\n",
    "accuracies = [np.mean(scores)*100 for scores in score_list]\n",
    "#print(f'{j} embeddings did not pass the threshold')\n",
    "#return accuracies\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6871dd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.figure(figsize=(12,7), dpi=300)\n",
    "plt.bar(neurotransmitters, accuracies)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Mean hard accuracy')\n",
    "#plt.title(f'Mean hard accuracies across classes - {model_size} DINOv2 - 140x140 images - Threshold = {distance_threshold} - Data augmentation: {data_aug}')\n",
    "plt.axhline(np.mean(accuracies), color='r', linestyle='--', label='Average')\n",
    "plt.axhline(y=(100/6), color='b', linestyle='--', label='Randomness')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,110])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524483d",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81468471",
   "metadata": {},
   "source": [
    "# Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "print('Done loading embeddings')\n",
    "\n",
    "#TODO:\n",
    "filtering = False\n",
    "\n",
    "if filtering:\n",
    "\n",
    "    LABELS = np.hstack([[neuro]*600 for neuro in neurotransmitters]).reshape(-1,1)\n",
    "\n",
    "    REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "    DATASET = few_shot.get_d_closest_elements(embeddings = DATA, \n",
    "                                            reference_emb = torch.from_numpy(REFS))\n",
    "\n",
    "else:\n",
    "    \n",
    "    LABELS = np.hstack([[neuro]*int((resize_size/14)**2 * 600) for neuro in neurotransmitters]).reshape(-1, 1)\n",
    "    \n",
    "    DATA = torch.cat(DATA)\n",
    "    DATA = DATA.reshape(-1, feat_dim)\n",
    "    \n",
    "    DATASET = list(zip(DATA, LABELS))\n",
    "\n",
    "DATASET = sample(DATASET, len(DATASET))\n",
    "\n",
    "SPLIT = int(len(DATASET)*0.2)\n",
    "TRAINING_SET = DATASET[SPLIT:]\n",
    "TEST_SET = DATASET[:SPLIT]\n",
    "\n",
    "one_hot_neurotransmitters = np.eye(len(neurotransmitters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as utils\n",
    "\n",
    "class Custom_LP_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 set):\n",
    "        if set == 'training':\n",
    "            self.data = TRAINING_SET\n",
    "        else:\n",
    "            self.data = TEST_SET\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding, label = self.data[idx]\n",
    "        label_idx = neurotransmitters.index(label[0])\n",
    "        return embedding, one_hot_neurotransmitters[label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ab8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size, test_batch_size = 50, 50\n",
    "\n",
    "training_dataset = Custom_LP_Dataset('training') \n",
    "test_dataset = Custom_LP_Dataset('test')\n",
    "\n",
    "training_loader = utils.DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7510ba",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b620d",
   "metadata": {},
   "source": [
    "# Define MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "def init_model(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "class MLP_Head(nn.Module):\n",
    "    def __init__(self, device, feat_dim):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.nb_outputs = 6\n",
    "        self.feat_dim = feat_dim\n",
    "        self.hidden_dims = self.feat_dim*np.array([3/4, 1/2, 1/4])\n",
    "        self.hidden_dims = self.hidden_dims.astype(int)\n",
    "\n",
    "        self.stack = nn.Sequential(nn.Linear(self.feat_dim,\n",
    "                                             self.hidden_dims[0]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(self.hidden_dims[0],\n",
    "                                             self.hidden_dims[1]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(self.hidden_dims[1],\n",
    "                                             self.hidden_dims[2]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(self.hidden_dims[2],\n",
    "                                             self.nb_outputs),\n",
    "                                   nn.Sigmoid())\n",
    "\n",
    "        self.apply(init_model)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea879e",
   "metadata": {},
   "source": [
    "## Call MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = MLP_Head(device=device, feat_dim=feat_dim)\n",
    "head.to(device)\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=3e-4)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e494ff",
   "metadata": {},
   "source": [
    "# Plot MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(head, input_size=(1,feat_dim), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515c93a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e79500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use(epochs):\n",
    "    head.train()\n",
    "    loss_list = []\n",
    "    prediction_list = []\n",
    "    test_accuracies = []\n",
    "    for _ in tqdm(range(epochs), desc=f'Epoch:'):\n",
    "        epoch_loss_list = []\n",
    "        proportion_list = []\n",
    "        for embeddings, one_hot_gts in tqdm(training_loader, desc='Training', leave=False):\n",
    "            embeddings = embeddings.to(device)\n",
    "            output = head(embeddings).to(torch.float64)\n",
    "            \n",
    "            gt = one_hot_gts\n",
    "            gt = gt.to(device)\n",
    "            loss=0\n",
    "            for out, true in zip(output,gt):\n",
    "                loss += loss_fn(out,true)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss_list.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            proportion_list.append(one_hot_gts)\n",
    "                \n",
    "        loss_list.append(np.mean(epoch_loss_list))\n",
    "\n",
    "        head.eval()\n",
    "        with torch.no_grad():\n",
    "            score = 0\n",
    "            total = 0\n",
    "            for embeddings, one_hot_gts in tqdm(test_loader, desc='Testing', leave=False):\n",
    "                embeddings = embeddings.to(device)\n",
    "                outputs = head(embeddings) # shape (batch_size, nb_classes)\n",
    "                \n",
    "                for output, one_hot_gt in zip(outputs, one_hot_gts):\n",
    "                    predicted_idx = torch.argmax(output).item()\n",
    "                    true_idx = torch.argmax(one_hot_gt).item()\n",
    "                    prediction_list.append([predicted_idx, true_idx])\n",
    "                    \n",
    "                    if predicted_idx == true_idx:\n",
    "                        score += 1\n",
    "                    total += 1\n",
    "                batch_score = 100*score/total\n",
    "                test_accuracies.append(batch_score)\n",
    "\n",
    "    return loss_list, proportion_list, prediction_list, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ab5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "loss_list, proportion_list, prediction_list, test_accuracies = use(epochs) # (pred, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3785c58",
   "metadata": {},
   "source": [
    "# Class proportions during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa85bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = []\n",
    "for e in proportion_list: gts.extend(e)\n",
    "gts = np.array(gts)\n",
    "\n",
    "vectors, counts = np.unique(gts, axis=0, return_counts=True)\n",
    "positions = [np.where(np.all(one_hot_neurotransmitters == v, axis=1)) for v in vectors]\n",
    "\n",
    "proportions = np.zeros((len(neurotransmitters),1))\n",
    "for count, position in zip(counts, positions): proportions[position] = count\n",
    "\n",
    "proportions = 100*proportions/int(np.sum(proportions))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4), dpi=150)\n",
    "\n",
    "img = ax.imshow(proportions.T, cmap='RdYlGn')\n",
    "\n",
    "for k, prop in enumerate(proportions):\n",
    "    text = ax.text(x=k, y=0, s=f'{round(proportions[k].item(), ndigits=2)}%\\n({int(counts[k])})', ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "ax.set_xticks(range(len(neurotransmitters)), labels=neurotransmitters, rotation=-45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "ax.text(5.7, 0, f'({int(np.sum(counts))})', va='center', ha='left', color='black')\n",
    "\n",
    "ax.set_title('Class proportions')\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "fig.colorbar(img, ax=ax, orientation='horizontal', label='Proportion')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225436ad",
   "metadata": {},
   "source": [
    "# Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((len(neurotransmitters), len(neurotransmitters)))\n",
    "for pred in prediction_list:\n",
    "    truth = pred[1]\n",
    "    prediction = pred[0]\n",
    "    confusion_matrix[truth, prediction] += 1\n",
    "    \n",
    "initial_confusion_matrix = confusion_matrix.copy()\n",
    "    \n",
    "total_list = []\n",
    "for row in confusion_matrix:\n",
    "    total = sum(row)\n",
    "    total_list.append(row)\n",
    "    row /= total\n",
    "confusion_matrix=100*confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7), dpi=150)\n",
    "\n",
    "import seaborn as sns\n",
    "im = ax.imshow(confusion_matrix, cmap='YlGn')\n",
    "\n",
    "ax.set_yticks(range(len(neurotransmitters)), labels=neurotransmitters)\n",
    "ax.set_xticks(range(len(neurotransmitters)), labels=neurotransmitters, rotation=-45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "for i in range(len(neurotransmitters)):\n",
    "    for j in range(len(neurotransmitters)):\n",
    "        text = ax.text(j, i, s=f'{round(confusion_matrix[i, j], ndigits=2)}%\\n({int(initial_confusion_matrix[i,j])})', ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "for i, row in enumerate(initial_confusion_matrix):\n",
    "    ax.text(5.75, i, f'({int(sum(row))})',\n",
    "            va='center', ha='left', color='black')\n",
    "    \n",
    "for j, row in enumerate(initial_confusion_matrix.T):\n",
    "    ax.text(j, 5.75, f'({int(sum(row))})',\n",
    "            va='center', ha='center', color='black')\n",
    "\n",
    "ax.text(5.75, 5.75, f'({int(np.sum(initial_confusion_matrix))})',\n",
    "            va='center', ha='left', color='black')\n",
    "\n",
    "fig.tight_layout()\n",
    "ax.set_title(f'Confusion matrix for filtered class-wise predictions - 20% Dataset - Epochs={epochs}')\n",
    "\n",
    "#fig.colorbar(im, ax=ax, orientation='vertical', label='Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f4da8",
   "metadata": {},
   "source": [
    "# General results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(epochs)]\n",
    "fig, ax1 = plt.subplots(figsize=(5,5), dpi=150)\n",
    "ax2 = ax1.twinx()\n",
    "lns1 = ax1.plot(x, loss_list, label='Train loss')\n",
    "ax1.set_ylim(0,max(loss_list)*1.05)\n",
    "lns2 = ax2.plot(x, test_accuracies, label='Test accuracy', color='red')\n",
    "ax2.set_ylim(0,105)\n",
    "\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b836307",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df74b7",
   "metadata": {},
   "source": [
    "# Freeze MLP Head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62883542",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in head.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406785b",
   "metadata": {},
   "source": [
    "# Load DINOv2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47529ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', f'dinov2_vit{model_size[0]}14_reg')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1dc283",
   "metadata": {},
   "source": [
    "# Freeze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce78c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b7f49",
   "metadata": {},
   "source": [
    "# Plot DINOv2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(model, input_size=(1,3,518,518), expand_nested=True, graph_dir='TB', strict=False, depth=10)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13567916",
   "metadata": {},
   "source": [
    "# Define adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AdaptMLP(nn.Module):\n",
    "    def __init__(self, device, original_mlp, in_dim, mid_dim, dropout=0.0, s=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.original_mlp = original_mlp # original MLP block\n",
    "        \n",
    "        # down --> non linear --> up\n",
    "        self.down_proj = nn.Linear(in_dim, mid_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.up_proj = nn.Linear(mid_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = s # scaling factor\n",
    "        \n",
    "        # initialization\n",
    "        nn.init.kaiming_uniform_(self.down_proj.weight)\n",
    "        nn.init.zeros_(self.up_proj.weight)\n",
    "        nn.init.zeros_(self.down_proj.bias)\n",
    "        nn.init.zeros_(self.up_proj.bias)\n",
    "        \n",
    "        # freeze original MLP\n",
    "        for p in self.original_mlp.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        down = self.down_proj(x)\n",
    "        down = self.act(down)\n",
    "        down = self.dropout(down)\n",
    "        up = self.up_proj(down)\n",
    "\n",
    "        output = self.original_mlp(x) + up * self.scale\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa488b15",
   "metadata": {},
   "source": [
    "# Change MLP blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(list(model.blocks))):\n",
    "\n",
    "    mlp = nn.Sequential(model.blocks[k].norm2,\n",
    "                        model.blocks[k].mlp,\n",
    "                        model.blocks[k].ls2)\n",
    "    in_dim = model.blocks[k].norm2.normalized_shape[0]\n",
    "    mid_dim = int(model.blocks[k].norm2.normalized_shape[0]/4)\n",
    "    \n",
    "    adapter = AdaptMLP(device, mlp, in_dim, mid_dim)\n",
    "\n",
    "    model.blocks[k].mlp = adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aff08d",
   "metadata": {},
   "source": [
    "# Plot Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46adf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(adapter, input_size=(1,feat_dim), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96499c4c",
   "metadata": {},
   "source": [
    "# Add MLP Head to modified DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_model = nn.Sequential(model, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe03c4",
   "metadata": {},
   "source": [
    "# Plot Augmented Model + MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a08dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(augmented_model, input_size=(1,3,518,518), expand_nested=True, depth=10)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0cc01",
   "metadata": {},
   "source": [
    "# Number of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec76d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [p for p in augmented_model.parameters() if p.requires_grad]\n",
    "params = sum([np.prod(p.size()) for p in trainable_params])\n",
    "\n",
    "frozen_params_list = [p for p in augmented_model.parameters() if not p.requires_grad]\n",
    "frozen_params = sum([np.prod(p.size()) for p in frozen_params_list])\n",
    "\n",
    "total_params = params + frozen_params\n",
    "\n",
    "print(f'Proportion of trainable parameters: {params / total_params * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea93299",
   "metadata": {},
   "source": [
    "# Dataset for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e275794",
   "metadata": {},
   "outputs": [],
   "source": [
    "files, _ = zip(*get_fnames())\n",
    "\n",
    "IMAGES = []\n",
    "for file in tqdm(files, desc='Loading images'):\n",
    "    im = resize_hdf_image(load_image(file)[0])\n",
    "    stack = np.concatenate([im, im, im], axis=2)\n",
    "    IMAGES.append(stack)\n",
    "IMAGES = np.array(IMAGES).transpose(0,3,1,2)\n",
    "\n",
    "FT_LABELS = np.hstack([[neuro]*600 for neuro in neurotransmitters]).reshape(-1,1)\n",
    "\n",
    "FT_DATASET = list(zip(IMAGES, LABELS))\n",
    "\n",
    "FT_DATASET = sample(FT_DATASET, len(FT_DATASET))\n",
    "\n",
    "FT_SPLIT = int(len(FT_DATASET)*0.2)\n",
    "FT_TRAINING_SET = FT_DATASET[FT_SPLIT:]\n",
    "FT_TEST_SET = FT_DATASET[:FT_SPLIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_FT_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 set):\n",
    "        if set == 'training':\n",
    "            self.data = FT_TRAINING_SET\n",
    "        else:\n",
    "            self.data = FT_TEST_SET\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        label_idx = neurotransmitters.index(label[0])\n",
    "        return image, one_hot_neurotransmitters[label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train_batch_size, ft_test_batch_size = 1, 1\n",
    "\n",
    "ft_training_dataset = Custom_FT_Dataset('training') \n",
    "ft_test_dataset = Custom_FT_Dataset('test')\n",
    "\n",
    "ft_training_loader = utils.DataLoader(ft_training_dataset, batch_size=ft_train_batch_size, shuffle=True, pin_memory=True)\n",
    "ft_test_loader = utils.DataLoader(ft_test_dataset, batch_size=ft_test_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3330c",
   "metadata": {},
   "source": [
    "# Training augmented model with frozen MLP Head (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219d0d2",
   "metadata": {},
   "source": [
    "# UMAP Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS)\n",
    "\n",
    "idx = 1\n",
    "\n",
    "EX_EMBEDDINGS = EMBEDDINGS[idx].reshape(-1, feat_dim)\n",
    "EX_REF = torch.tensor(REFS[0])\n",
    "\n",
    "embeddings_and_ref = np.vstack([EX_REF, EX_EMBEDDINGS])\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "semantic_distances = compute_similarity_matrix(EX_REF, EX_EMBEDDINGS)\n",
    "\n",
    "plt.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "plt.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='red', marker='o')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_optimizer = torch.optim.Adam(augmented_model.parameters(), lr=3e-4)\n",
    "augmented_model.to(device)\n",
    "ft_loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86257117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(epochs):\n",
    "    augmented_model.train()\n",
    "    loss_list = []\n",
    "    prediction_list = []\n",
    "    test_accuracies = []\n",
    "    for _ in tqdm(range(epochs), desc=f'Epoch:'):\n",
    "        epoch_loss_list = []\n",
    "        for images, one_hot_gts in tqdm(ft_training_loader, desc='Training', leave=False):\n",
    "            images = images.to(torch.float32).to(device)\n",
    "            output = augmented_model(images).to(torch.float64)\n",
    "            \n",
    "            gt = one_hot_gts\n",
    "            gt = gt.to(device)\n",
    "            loss=0\n",
    "            for out, true in zip(output,gt):\n",
    "                loss += ft_loss_fn(out,true)\n",
    "                \n",
    "            loss.backward()\n",
    "            ft_optimizer.step()\n",
    "            ft_optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss_list.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        loss_list.append(np.mean(epoch_loss_list))\n",
    "\n",
    "        augmented_model.eval()\n",
    "        with torch.no_grad():\n",
    "            score = 0\n",
    "            total = 0\n",
    "            for images, one_hot_gts in tqdm(ft_test_loader, desc='Testing', leave=False):\n",
    "                \n",
    "                images = images.to(torch.float32).to(device)\n",
    "                outputs = augmented_model(images) # shape (batch_size, nb_classes)\n",
    "                \n",
    "                for output, one_hot_gt in zip(outputs, one_hot_gts):\n",
    "                    predicted_idx = torch.argmax(output).item()\n",
    "                    true_idx = torch.argmax(one_hot_gt).item()\n",
    "                    prediction_list.append([predicted_idx, true_idx])\n",
    "                    \n",
    "                    if predicted_idx == true_idx:\n",
    "                        score += 1\n",
    "                    total += 1\n",
    "                batch_score = 100*score/total\n",
    "            test_accuracies.append(batch_score)\n",
    "\n",
    "    return loss_list, prediction_list, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807562f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "loss_list, prediction_list, test_accuracies = fine_tuning(nb_epochs) # (pred, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a44dd",
   "metadata": {},
   "source": [
    "# UMAP After:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce513a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a hook to get representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326011da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(epochs)]\n",
    "fig, ax1 = plt.subplots(figsize=(5,5), dpi=150)\n",
    "ax2 = ax1.twinx()\n",
    "lns1 = ax1.plot(x, loss_list, label='Train loss')\n",
    "ax1.set_ylim(0,max(loss_list)*1.05)\n",
    "lns2 = ax2.plot(x, test_accuracies, label='Test accuracy', color='red')\n",
    "ax2.set_ylim(0,105)\n",
    "\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc=0)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
