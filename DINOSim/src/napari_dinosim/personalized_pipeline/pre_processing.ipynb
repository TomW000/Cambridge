{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py \n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: For LINUX:\n",
    "dataset_path = '/home/tomwelch/Cambridge/Datasets/neurotransmitter_data' #@param {type:\"string\"}\n",
    "\n",
    "#TODO: For MAC:\n",
    "#dataset_path = '/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/neurotransmitter_data'\n",
    "\n",
    "dates = glob(os.path.join(dataset_path, '*'))\n",
    "neurotransmitters = list(map(lambda x: os.path.basename(os.path.normpath(x)), glob(os.path.join(dates[0], '*')))) #@param {type:\"string\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(path):\n",
    "    with h5py.File(path) as f:\n",
    "        pre, post = f['annotations/locations'][:]/8\n",
    "        x_pre, y_pre, z_pre = pre[0].astype(int), pre[1].astype(int), pre[2].astype(int)\n",
    "        x_post, y_post, z_post = post[0].astype(int), post[1].astype(int), post[2].astype(int)\n",
    "        direction_vector = np.array([x_pre - x_post, y_pre - y_post, z_pre - z_post])\n",
    "        main_axis = np.argmax(np.abs(direction_vector))\n",
    "        norm = np.linalg.norm(direction_vector)\n",
    "        coord_list = ['x','y','z']\n",
    "        return coord_list[main_axis], norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics():\n",
    "    all_stats = []\n",
    "    for date in dates:\n",
    "        for neuro in neurotransmitters:\n",
    "            neuro_stats = []\n",
    "            path_list = glob(os.path.join(date, neuro, '*.hdf*'))\n",
    "            path_list.sort()\n",
    "            for path in path_list:\n",
    "                _, norm = get_direction(path)\n",
    "                neuro_stats.append(norm)\n",
    "            all_stats.append(np.mean(neuro_stats))\n",
    "    stats = []\n",
    "    nb_neuro = len(neurotransmitters)\n",
    "    for k in range(nb_neuro):\n",
    "        stats.append([np.mean(all_stats[k]+all_stats[k+nb_neuro]), neurotransmitters[k].capitalize()])\n",
    "    plt.figure(figsize=(12,7), dpi=300)\n",
    "    plt.bar([x[1] for x in stats], [x[0] for x in stats])\n",
    "    plt.ylabel('Mean Pre-Post Distance')\n",
    "    plt.title('Mean Pre-Post Distance by Neurotransmitter')\n",
    "    mean = np.mean([x[0] for x in stats])\n",
    "    plt.axhline(mean, label=f'{mean: .1f}', color='red', linestyle='dashed')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_image(path):\n",
    "    main_axis, norm = get_direction(path)\n",
    "    norm = round(norm)\n",
    "    with h5py.File(path) as f:\n",
    "        pre, _ = f['annotations/locations'][:]/8\n",
    "        x, y, z = [pre[i].astype(int) for i in range(3)]\n",
    "        \n",
    "        slice_volume = f['volumes/raw'][:]\n",
    "        \n",
    "        # Get volume dimensions to ensure we don't go out of bounds\n",
    "        dim_x, dim_y, dim_z = slice_volume.shape\n",
    "        \n",
    "        if main_axis == 'x':\n",
    "            y1, y2 = max(0, y-norm), min(dim_y, y+norm+1)\n",
    "            z1, z2 = max(0, z-norm), min(dim_z, z+norm+1)\n",
    "            return slice_volume[x, y1:y2, z1:z2], slice_volume[x,:,:] \n",
    "        elif main_axis == 'y':\n",
    "            x1, x2 = max(0, x-norm), min(dim_x, x+norm+1)\n",
    "            z1, z2 = max(0, z-norm), min(dim_z, z+norm+1)\n",
    "            return slice_volume[x1:x2, y, z1:z2], slice_volume[:,y,:]\n",
    "        else:  # main_axis == 'z'\n",
    "            x1, x2 = max(0, x-norm), min(dim_x, x+norm+1)\n",
    "            y1, y2 = max(0, y-norm), min(dim_y, y+norm+1)\n",
    "            return slice_volume[x1:x2, y1:y2, z], slice_volume[:,:,z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cropped_image(n):\n",
    "    for date in dates:\n",
    "            for neuro in neurotransmitters:\n",
    "                fnames = glob(os.path.join(date, neuro, '*.hdf*'))\n",
    "                fnames.sort()\n",
    "                for file in fnames[:n]:\n",
    "                    img_c, img_o = get_cropped_image(file)\n",
    "                    plt.figure(figsize=(12,7), dpi=300)\n",
    "                    plt.suptitle(f'{neuro.capitalize()}')\n",
    "                    plt.subplot(121)\n",
    "                    plt.imshow(img_o, cmap='gray')\n",
    "                    plt.title(f'Original image')\n",
    "                    plt.subplot(122)\n",
    "                    plt.imshow(img_c, cmap='gray')\n",
    "                    plt.title('Cropped image')\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomwelch/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from setup import model, Trans, umap, torch, tqdm, np, px, PCA, os, glob\n",
    "from perso_utils import get_processed_image, get_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(batch_size=1):\n",
    "\n",
    "    transform = Trans.Compose([\n",
    "        Trans.ToTensor(),\n",
    "        Trans.Resize((224, 224)),\n",
    "        Trans.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    fnames, labels_l = get_fnames()\n",
    "    labels = labels_l[:100]\n",
    "    latent, img_batch = [], []\n",
    "    \n",
    "    for file in tqdm(fnames[:100], desc='Computing embeddings'):\n",
    "        try:\n",
    "            # Get and process the image\n",
    "            img_c = get_processed_image(file)[3][0,...,0]\n",
    "            img_rgb = np.stack([img_c, img_c, img_c], axis=2)\n",
    "            img_tensor = transform(img_rgb).unsqueeze(0)  # Add batch dimension\n",
    "            img_batch.append(img_tensor)\n",
    "            \n",
    "            # Process batch when we reach batch_size\n",
    "            if len(img_batch) == batch_size:\n",
    "                batch_tensor = torch.cat(img_batch, dim=0)\n",
    "                with torch.no_grad():\n",
    "                    features = model(batch_tensor)\n",
    "                    latent.append(features.cpu().numpy())\n",
    "                img_batch = []  # Clear the batch\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Process any remaining images in the last batch\n",
    "    if img_batch:\n",
    "        batch_tensor = torch.cat(img_batch, dim=0)\n",
    "        with torch.no_grad():\n",
    "            features = model(batch_tensor)\n",
    "            latent.append(features.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches of features\n",
    "    if not latent:\n",
    "        return None, labels\n",
    "    return np.stack(latent), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 100/100 [00:05<00:00, 19.91it/s]\n"
     ]
    }
   ],
   "source": [
    "e, l = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=25)\n",
    "reduced_features = pca.fit_transform(e.squeeze())\n",
    "print(reduced_features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducer = umap.UMAP(\n",
    "            n_neighbors=5,\n",
    "            min_dist=0.01,\n",
    "            n_components=2,\n",
    "            metric='cosine',\n",
    "            )\n",
    "embedding = reducer.fit_transform(reduced_features)\n",
    "embedding.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
