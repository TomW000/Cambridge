{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae377bba",
      "metadata": {
        "id": "ae377bba",
        "papermill": {
          "duration": 0.004156,
          "end_time": "2022-05-03T01:17:20.222145",
          "exception": false,
          "start_time": "2022-05-03T01:17:20.217989",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Setup\n",
        "Import required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1531e42e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-03T01:17:23.974262Z",
          "iopub.status.busy": "2022-05-03T01:17:23.973981Z",
          "iopub.status.idle": "2022-05-03T01:17:27.209603Z",
          "shell.execute_reply": "2022-05-03T01:17:27.208919Z"
        },
        "id": "1531e42e",
        "outputId": "2ed378df-ffe9-4dae-d1f0-757ed690fee0",
        "papermill": {
          "duration": 3.242624,
          "end_time": "2022-05-03T01:17:27.211198",
          "exception": false,
          "start_time": "2022-05-03T01:17:23.968574",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from napari_dinosim.dinoSim_pipeline import *\n",
        "from napari_dinosim.utils import *\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "import h5py\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db9180c",
      "metadata": {
        "id": "4db9180c"
      },
      "outputs": [],
      "source": [
        "#@title Data and Processing Parameters\n",
        "\n",
        "#@markdown ### Data Settings\n",
        "dataset_path = '/home/tomwelch/Cambridge/Datasets/neurotransmitter_data' #@param {type:\"string\"}\n",
        "dates = os.listdir(dataset_path)#@param {type:\"string\"}\n",
        "neurotransmitters = os.listdir(os.path.join(dataset_path, dates[0])) #@param {type:\"string\"} \n",
        "\n",
        "#@markdown ### Post-processing Settings\n",
        "upsample = \"bilinear\" #@param {type:\"string\", options:[\"bilinear\", \"Nearest Neighbor\", \"None\"], value-map:{bilinear:\"bilinear\", \"Nearest Neighbor\": \"nearest\", None:None}}\n",
        "crop_shape = (512,512,1) #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown ### Model Input Settings\n",
        "#@markdown Should be multiple of model patch_size\n",
        "resize_size = 518 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e89d78",
      "metadata": {
        "id": "50e89d78"
      },
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83dad91e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image(path):\n",
        "    with h5py.File(path) as f:\n",
        "        pre, post = f['annotations/locations'][:]/8\n",
        "        x, y, z = pre[0].astype(int), pre[1].astype(int), pre[2].astype(int)\n",
        "        slice_volume = f['volumes/raw'][:][:,:,z]\n",
        "        return slice_volume, x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5303ac",
      "metadata": {
        "id": "dc5303ac",
        "outputId": "511ec121-4f3e-4ce3-fbec-1f25dede2501"
      },
      "outputs": [],
      "source": [
        "train_x_fnames = glob(os.path.join(dataset_path, dates[0], neurotransmitters[0], '*.hdf*'))\n",
        "train_x_fnames.sort()\n",
        "\n",
        "train_dataset = np.stack([load_image(p)[0] for p in train_x_fnames]).astype(np.uint8)\n",
        "train_dataset = train_dataset[...,np.newaxis] # add channel dim\n",
        "\n",
        "coordinates = np.stack([load_image(p)[1:] for p in train_x_fnames])\n",
        "\n",
        "print(f'N files:  {len(train_dataset)} \\t Shape: {train_dataset.shape[1:]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a44e30",
      "metadata": {
        "id": "91a44e30"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944b866d",
      "metadata": {
        "id": "944b866d"
      },
      "outputs": [],
      "source": [
        "# select model size\n",
        "model_size = 'small' #@param {type:\"string\", options:[\"small\", \"base\", \"large\", \"giant\"]}\n",
        "\n",
        "model_dims = {'small': 384, 'base': 768, 'large': 1024, 'giant': 1536}\n",
        "assert model_size in model_dims, f'Invalid model size: ({model_size})'\n",
        "model = torch.hub.load('facebookresearch/dinov2', f'dinov2_vit{model_size[0]}14_reg')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "feat_dim = model_dims[model_size]\n",
        "\n",
        "few_shot = DinoSim_pipeline(model, model.patch_size, device, get_img_processing_f(resize_size),\n",
        "                             feat_dim, dino_image_size=resize_size )\n",
        "print(\"Model loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fe52ff",
      "metadata": {
        "id": "58fe52ff"
      },
      "source": [
        "# Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77af1527",
      "metadata": {
        "id": "77af1527",
        "outputId": "67ceb6fc-cf6b-43ec-9941-dd5a539768e2"
      },
      "outputs": [],
      "source": [
        "# select reference points\n",
        "x, y = coordinates[0]+[-5,0]\n",
        "points = [(0, x, y),]\n",
        "z, x, y = zip(*points)\n",
        "\n",
        "plt.imshow(train_dataset[z[0]], 'gray')\n",
        "plt.scatter(x, y, c='r', marker='x')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929130e0",
      "metadata": {
        "id": "929130e0"
      },
      "source": [
        "# DinoSim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451d6729",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_kernel(size=3, sigma=1):\n",
        "\n",
        "    upper = size - 1\n",
        "    lower = -int(size / 2)\n",
        "\n",
        "    y, x = np.mgrid[lower:upper, lower:upper]\n",
        "\n",
        "    kernel = (1 / (2 * np.pi * sigma**2)) * np.exp(\n",
        "        -(x**2 + y**2) / (2 * sigma**2)\n",
        "    )\n",
        "    kernel = kernel / kernel.sum()\n",
        "\n",
        "    return kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "427bb10d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def torch_convolve(input, weights, mode=\"reflect\", cval=0.0, origin=0):\n",
        "    \"\"\"\n",
        "    Multidimensional convolution using PyTorch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : torch.Tensor\n",
        "        The input tensor to be convolved.\n",
        "    weights : torch.Tensor\n",
        "        Convolution kernel, with the same number of dimensions as the input.\n",
        "    mode : str, optional\n",
        "        Padding mode. Options are 'reflect', 'constant', 'replicate', or 'circular'.\n",
        "        Default is 'reflect'.\n",
        "    cval : float, optional\n",
        "        Value to fill past edges of input if `mode` is 'constant'. Default is 0.0.\n",
        "    origin : int, optional\n",
        "        Controls the origin of the input signal. Positive values shift the filter\n",
        "        to the right, and negative values shift the filter to the left. Default is 0.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result : torch.Tensor\n",
        "        The result of convolution of `input` with `weights`.\n",
        "    \"\"\"\n",
        "    # Ensure input is 4D (batch, channels, height, width)\n",
        "    if input.dim() == 2:  # Single channel 2D image\n",
        "        input = input.unsqueeze(0).unsqueeze(0)\n",
        "    elif input.dim() == 3:  # Add batch dimension if missing\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "    # Add channel dimension for weights if necessary\n",
        "    if weights.dim() == 2:\n",
        "        weights = weights.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply padding based on mode\n",
        "    padding = (\n",
        "        weights.shape[-1] // 2 - origin\n",
        "    )  # Adjust padding for origin shift\n",
        "    input_padded = F.pad(\n",
        "        input, (padding, padding, padding, padding), mode=mode, value=cval\n",
        "    )\n",
        "\n",
        "    # Perform convolution\n",
        "    result = F.conv2d(input_padded, weights)\n",
        "\n",
        "    return result.squeeze()  # Remove extra dimensions for output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c313a8d7",
      "metadata": {
        "id": "c313a8d7",
        "outputId": "1e413614-3632-45cf-9e12-a072ad267029"
      },
      "outputs": [],
      "source": [
        "#Post-processing\n",
        "kernel = gaussian_kernel(size=3, sigma=1)\n",
        "kernel = torch.tensor(kernel, dtype=torch.float32, device=device)\n",
        "filter_f = lambda x: torch_convolve(x, kernel)\n",
        "\n",
        "#few_shot.delete_precomputed_embeddings()\n",
        "# few_shot.delete_references()\n",
        "if not few_shot.emb_precomputed:\n",
        "    few_shot.pre_compute_embeddings(train_dataset, overlap = (0,0), padding=(0,0), crop_shape=crop_shape, verbose=True, batch_size=5)\n",
        "few_shot.set_reference_vector(list_coords=points)\n",
        "distances = few_shot.get_ds_distances_sameRef(verbose=False)\n",
        "predictions = few_shot.distance_post_processing(distances, filter_f, upsampling_mode=upsample)\n",
        "print(\"Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d84ab48",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions[6].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95164c2",
      "metadata": {
        "id": "d95164c2"
      },
      "source": [
        "# Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57395530",
      "metadata": {
        "id": "57395530",
        "outputId": "5d456320-11db-4707-cff7-6a1a61b53f81"
      },
      "outputs": [],
      "source": [
        "# select threshold\n",
        "threshold = 0.5\n",
        "\n",
        "for i in range(1):\n",
        "    input_img = train_dataset[i]\n",
        "    plt.figure(figsize=(12,7))\n",
        "    plt.subplot(131)\n",
        "    if input_img.shape[-1] == 1:\n",
        "        plt.imshow(input_img[...,0], 'gray')\n",
        "    else:\n",
        "        plt.imshow(input_img)\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(1-predictions[i], 'magma')\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(predictions[i] < threshold, 'gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51b2c37",
      "metadata": {
        "id": "d51b2c37"
      },
      "outputs": [],
      "source": [
        "def get_bbox(predictions, threshold):\n",
        "    bbox_list = []\n",
        "    failed = []\n",
        "    try:\n",
        "        for i in range(predictions.shape[0]):\n",
        "            GT = (predictions[i] < threshold).astype(np.uint8)\n",
        "            if np.sum(GT) == 0:\n",
        "                failed.append(i)\n",
        "                continue\n",
        "            bbox_1x, bbox_1y = min(np.where(GT == 1)[1])-5, min(np.where(GT == 1)[0])-5\n",
        "            bbox_2x, bbox_2y = max(np.where(GT == 1)[1])+5, max(np.where(GT == 1)[0])+5\n",
        "            bbox_list.append(((bbox_1x, bbox_2x), (bbox_1y, bbox_2y)))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in bbox nb.{i}: {e}\")\n",
        "    \n",
        "    print(f\"{len(failed)/predictions.shape[0]*100}% of images did not pass the threshold\")\n",
        "    return bbox_list, len(failed)/predictions.shape[0]*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c074779e",
      "metadata": {},
      "outputs": [],
      "source": [
        "bboxes, _ = get_bbox(predictions, 0.35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b23af1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(threshold):\n",
        "    return get_bbox(predictions, threshold)[1]\n",
        "\n",
        "plt.figure(figsize=(12,7), dpi=300)\n",
        "thresholds = np.arange(0, 1.05, 0.05)\n",
        "plt.plot(thresholds, [f(threshold)for threshold in thresholds])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89a22c2d",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "66f1c3a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tomwelch/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from napari_dinosim.dinoSim_pipeline import *\n",
        "from napari_dinosim.utils import *\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "import h5py\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms.v2.functional as T\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "afbf6362",
      "metadata": {},
      "outputs": [],
      "source": [
        "list_neurotransmitters = {0:'acetylcholine',\n",
        "                          1:'serotonin',\n",
        "                          2:'gaba',\n",
        "                          3:'glutamate',\n",
        "                          4:'octopamine',\n",
        "                          5:'dopamine'}\n",
        "\n",
        "list_dates = {0: 'sylee_neurotrans_cubes_18Feb2025', \n",
        "              1:'sylee_neurotrans_cubes_24Mar2025'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7fe149d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_img_processing_f(\n",
        "    resize_size=224,\n",
        "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std=(0.229, 0.224, 0.225),\n",
        "):\n",
        "    # input  tensor: [(b),h,w,c]\n",
        "    # output tensor: [(b),c,h,w]\n",
        "    def _img_processing_f(x):\n",
        "        if len(x.shape) == 4:\n",
        "            if x.shape[-1] == 1:\n",
        "                x = x.repeat(1, 1, 1, 3)\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            if x.shape[-1] == 1:\n",
        "                x = x.repeat(1, 1, 3)\n",
        "            x = x.permute(2, 0, 1)\n",
        "        x = T.resize(\n",
        "            x, resize_size, interpolation=interpolation, antialias=True\n",
        "        )\n",
        "        x = T.normalize(x, mean=mean, std=std)\n",
        "        return x\n",
        "\n",
        "    return _img_processing_f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22134e93",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image(path):\n",
        "    with h5py.File(path) as f:\n",
        "        pre, post = f['annotations/locations'][:]/8\n",
        "        x, y, z = pre[0].astype(int), pre[1].astype(int), pre[2].astype(int)\n",
        "        slice_volume = f['volumes/raw'][:][:,:,z]\n",
        "        return slice_volume, x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "89839e1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_kernel(size=3, sigma=1):\n",
        "\n",
        "    upper = size - 1\n",
        "    lower = -int(size / 2)\n",
        "\n",
        "    y, x = np.mgrid[lower:upper, lower:upper]\n",
        "\n",
        "    kernel = (1 / (2 * np.pi * sigma**2)) * np.exp(\n",
        "        -(x**2 + y**2) / (2 * sigma**2)\n",
        "    )\n",
        "    kernel = kernel / kernel.sum()\n",
        "\n",
        "    return kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "44cd3f24",
      "metadata": {},
      "outputs": [],
      "source": [
        "def torch_convolve(input, weights, mode=\"reflect\", cval=0.0, origin=0):\n",
        "    if input.dim() == 2:\n",
        "        input = input.unsqueeze(0).unsqueeze(0)\n",
        "    elif input.dim() == 3: \n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "    if weights.dim() == 2:\n",
        "        weights = weights.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    padding = (\n",
        "        weights.shape[-1] // 2 - origin\n",
        "    ) \n",
        "    input_padded = F.pad(\n",
        "        input, (padding, padding, padding, padding), mode=mode, value=cval\n",
        "    )\n",
        "    result = F.conv2d(input_padded, weights)\n",
        "\n",
        "    return result.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bc7a1f74",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bbox(predictions, threshold):\n",
        "    bbox_list = []\n",
        "    failed = []\n",
        "    try:\n",
        "        for i in range(predictions.shape[0]):\n",
        "            GT = (predictions[i] < threshold).astype(np.uint8)\n",
        "            if np.sum(GT) == 0:\n",
        "                failed.append(i)\n",
        "                continue\n",
        "            bbox_1x, bbox_1y = min(np.where(GT == 1)[1])-5, min(np.where(GT == 1)[0])-5\n",
        "            bbox_2x, bbox_2y = max(np.where(GT == 1)[1])+5, max(np.where(GT == 1)[0])+5\n",
        "            bbox_list.append(((bbox_1x, bbox_2x), (bbox_1y, bbox_2y)))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in bbox nb.{i}: {e}\")\n",
        "    \n",
        "    #print(f\"{len(failed)/predictions.shape[0]*100}% of images did not pass the threshold\")\n",
        "    return bbox_list, len(failed)/predictions.shape[0]*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ba77d616",
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(predictions, threshold):\n",
        "    return get_bbox(predictions, threshold)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "da23ef5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Data and Processing Parameters\n",
        "\n",
        "#@markdown ### Data Settings\n",
        "dataset_path = '/home/tomwelch/Cambridge/Datasets/neurotransmitter_data' #@param {type:\"string\"}\n",
        "dates = os.listdir(dataset_path)#@param {type:\"string\"}\n",
        "neurotransmitters = os.listdir(os.path.join(dataset_path, dates[0])) #@param {type:\"string\"} \n",
        "\n",
        "#@markdown ### Post-processing Settings\n",
        "upsample = \"bilinear\" #@param {type:\"string\", options:[\"bilinear\", \"Nearest Neighbor\", \"None\"], value-map:{bilinear:\"bilinear\", \"Nearest Neighbor\": \"nearest\", None:None}}\n",
        "crop_shape = (512,512,1) #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown ### Model Input Settings\n",
        "#@markdown Should be multiple of model patch_size\n",
        "resize_size = 518 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "70a10b2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded\n"
          ]
        }
      ],
      "source": [
        "# select model size\n",
        "model_size = 'small' #@param {type:\"string\", options:[\"small\", \"base\", \"large\", \"giant\"]}\n",
        "\n",
        "model_dims = {'small': 384, 'base': 768, 'large': 1024, 'giant': 1536}\n",
        "assert model_size in model_dims, f'Invalid model size: ({model_size})'\n",
        "model = torch.hub.load('facebookresearch/dinov2', f'dinov2_vit{model_size[0]}14_reg')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "feat_dim = model_dims[model_size]\n",
        "\n",
        "few_shot = DinoSim_pipeline(model, model.patch_size, device, get_img_processing_f(resize_size),\n",
        "                             feat_dim, dino_image_size=resize_size )\n",
        "print(\"Model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ab872827",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DINOSim():\n",
        "    '''\n",
        "    Runs few-shot DINO-based detection over all dates and 6 neurotransmitters,\n",
        "    and plots success rate over thresholds in a 2x3 subplot grid.\n",
        "    '''\n",
        "    thresholds = np.arange(0, 1.05, 0.05)\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, neuro in enumerate(neurotransmitters):\n",
        "        all_success_rates = []\n",
        "\n",
        "        for date in dates:\n",
        "            # Load files\n",
        "            train_x_fnames = glob(os.path.join(dataset_path, date, neuro, '*.hdf*'))\n",
        "            train_x_fnames.sort()\n",
        "\n",
        "            # Skip if no data\n",
        "            if len(train_x_fnames) == 0:\n",
        "                print(f\"Skipping {neuro} on {dates[date]} (no data)\")\n",
        "                continue\n",
        "\n",
        "            train_dataset = np.stack([load_image(p)[0] for p in train_x_fnames])\n",
        "            train_dataset = train_dataset[..., np.newaxis]\n",
        "            coordinates = np.stack([load_image(p)[1:] for p in train_x_fnames])\n",
        "            points = [(0, coord[0], coord[1]) for coord in coordinates]\n",
        "\n",
        "            # Convolution kernel\n",
        "            kernel = gaussian_kernel(size=3, sigma=1)\n",
        "            kernel = torch.tensor(kernel, dtype=torch.float32, device=device)\n",
        "            filter_f = lambda x: torch_convolve(x, kernel)\n",
        "\n",
        "            # Few-shot setup\n",
        "            few_shot.delete_precomputed_embeddings()\n",
        "            few_shot.delete_references()\n",
        "            if not few_shot.emb_precomputed:\n",
        "                few_shot.pre_compute_embeddings(train_dataset[:50], \n",
        "                                                overlap=(0, 0),\n",
        "                                                padding=(0, 0),\n",
        "                                                crop_shape=crop_shape, \n",
        "                                                verbose=False,\n",
        "                                                batch_size=5)\n",
        "            print('Done')\n",
        "            predictions = []\n",
        "\n",
        "            for point in tqdm(points, desc=f'{neuro} on {date}'):\n",
        "                few_shot.set_reference_vector(list_coords=[(0, point[0], point[1])])\n",
        "                distances = few_shot.get_ds_distances_sameRef(verbose=False)\n",
        "                pred = few_shot.distance_post_processing(distances, filter_f, upsampling_mode=upsample)\n",
        "                predictions.append(pred)\n",
        "\n",
        "            predictions = np.array(predictions)\n",
        "            failure_rate = [f(predictions, t) for t in thresholds]\n",
        "            all_failure_rates.append(failure_rate)\n",
        "\n",
        "        # Average across dates\n",
        "        if all_success_rates:\n",
        "            mean_rates = np.mean(all_success_rates, axis=0)\n",
        "            axs[i].plot(thresholds, mean_rates, label=neuro)\n",
        "            axs[i].set_title(f'{neuro}')\n",
        "            axs[i].set_xlabel('Threshold')\n",
        "            axs[i].set_ylabel('Success Rate')\n",
        "            axs[i].grid(True)\n",
        "            axs[i].legend()\n",
        "\n",
        "    plt.suptitle('Success Rate vs Threshold for Neurotransmitters', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9c8643b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputing embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "acetylcholine on sylee_neurotrans_cubes_18Feb2025: 100%|██████████| 300/300 [00:54<00:00,  5.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputing embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "acetylcholine on sylee_neurotrans_cubes_24Mar2025: 100%|██████████| 300/300 [00:52<00:00,  5.76it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m DINOSim()\n",
            "\u001b[1;32m/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(predictions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m    # Define success metric function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m    def f(preds, threshold):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m        return np.mean(preds > threshold)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     success_rates \u001b[39m=\u001b[39m [f(predictions, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m thresholds]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     all_success_rates\u001b[39m.\u001b[39mappend(success_rates)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Average across dates\u001b[39;00m\n",
            "\u001b[1;32m/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(predictions, threshold):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m get_bbox(predictions, threshold)[\u001b[39m1\u001b[39m]\n",
            "\u001b[1;32m/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     failed\u001b[39m.\u001b[39mappend(i)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m bbox_1x, bbox_1y \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(np\u001b[39m.\u001b[39mwhere(GT \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m])\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, \u001b[39mmin\u001b[39m(np\u001b[39m.\u001b[39mwhere(GT \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m])\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m bbox_2x, bbox_2y \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(np\u001b[39m.\u001b[39mwhere(GT \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m, \u001b[39mmax\u001b[39m(np\u001b[39m.\u001b[39mwhere(GT \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/DINOSim/napari-dinoSim/src/dinoSim_example_original.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m bbox_list\u001b[39m.\u001b[39mappend(((bbox_1x, bbox_2x), (bbox_1y, bbox_2y)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x1000 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "DINOSim()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "cell_metadata_filter": "colab,colab_type,id,-all",
      "formats": "ipynb,py:percent",
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 24.542489,
      "end_time": "2022-05-03T01:17:43.180500",
      "environment_variables": {},
      "exception": null,
      "input_path": "course_UvA-DL/11-vision-transformer/Vision_Transformer.ipynb",
      "output_path": ".notebooks/course_UvA-DL/11-vision-transformer.ipynb",
      "parameters": {},
      "start_time": "2022-05-03T01:17:18.638011",
      "version": "2.3.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7df9e58a0e73894e5865375860f80f2b4596667997f401ebab766884108c467"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
