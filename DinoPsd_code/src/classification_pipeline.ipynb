{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0924a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d004fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, curated_idx, few_shot_transforms,  embeddings_path, model\n",
    "from setup import tqdm, torch, np, os, plt, tqdm, gc, sample\n",
    "from analysis_utils import display_hdf_image_grid, resize_hdf_image, get_augmented_coordinates\n",
    "from setup import cosine_similarity, euclidean_distances\n",
    "from perso_utils import get_fnames, load_image, get_latents\n",
    "from DinoPsd import DinoPsd_pipeline\n",
    "from DinoPsd_utils import get_img_processing_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fba314",
   "metadata": {},
   "source": [
    "### Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = DinoPsd_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n",
    "\n",
    "files, labels = zip(*get_fnames()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_refs = compute_ref_embeddings(True, os.path.join(embeddings_path, 'giant_mean_ref_518_Aug=False_k=10'))\n",
    "#mean_refs = compute_ref_embeddings(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=True):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "            imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "            imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "            good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "            \n",
    "            good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "            good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "            good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "        unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "        for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "            \n",
    "            # Pre-compute embeddings\n",
    "            few_shot.pre_compute_embeddings(\n",
    "                dataset,  # Pass numpy array of images\n",
    "                overlap=(0.5, 0.5),\n",
    "                padding=(0, 0),\n",
    "                crop_shape=(518, 518, 1),\n",
    "                verbose=True,\n",
    "                batch_size=10\n",
    "            )\n",
    "            \n",
    "            # Set reference vectors\n",
    "            few_shot.set_reference_vector(coordinates, filter=None)\n",
    "            ref = few_shot.get_refs()\n",
    "            \n",
    "            # Get closest elements - using the correct method name\n",
    "            close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "            k_labels =  [batch_label for _ in range(k)]\n",
    "\n",
    "            \n",
    "            # Convert to numpy for storing\n",
    "            close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "            \n",
    "            filtered_latent_list.append(close_embedding_np)\n",
    "            filtered_label_list.append(k_labels)\n",
    "            \n",
    "            # Clean up to free memory\n",
    "            few_shot.delete_precomputed_embeddings()\n",
    "            few_shot.delete_references()\n",
    "\n",
    "        mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\n",
    "        # Stack all embeddings and labels\n",
    "        ref_latents = np.vstack(filtered_latent_list)\n",
    "        ref_labels = np.hstack(filtered_label_list)\n",
    "        \n",
    "        torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7274",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_neurotransmitters = np.eye(len(neurotransmitters))\n",
    "#emb_labels = np.hstack([[neuro]*int((resize_size/14)**2 * 600) for neuro in neurotransmitters]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859a151",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Datasetwide Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ecbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_embeddings = compute_embeddings()\n",
    "#new_embeddings = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt')) # takes ~ 45 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef9b0c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute class-wise accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39697",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from analysis_utils import get_threshold\n",
    "\n",
    "def compute_accuracies(reference_embeddings = mean_refs, \n",
    "                       embeddings = new_embeddings,\n",
    "                       metric = euclidean_distances,\n",
    "                       distance_threshold = 0.01\n",
    "                       ):\n",
    "\n",
    "    batch_size = int(len(embeddings)/6)\n",
    "\n",
    "    for n, i in tqdm(enumerate(range(0, len(embeddings), batch_size))):\n",
    "        batch = embeddings[i:i+batch_size]\n",
    "\n",
    "        #embeddings = embeddings.reshape(-1, feat_dim)\n",
    "        similarity_matrix = metric(reference_embeddings, batch)\n",
    "        similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "        threshold = get_threshold(similarity_matrix_normalized, 0.9)\n",
    "        similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= threshold, similarity_matrix_normalized, 0)\n",
    "\n",
    "        batch_score_list = []\n",
    "        for k in range(batch_size):\n",
    "\n",
    "            column = similarity_matrix_normalized_filtered[:,k]\n",
    "            j=0\n",
    "            if sum(column) == 0:\n",
    "                j+=1\n",
    "            else:\n",
    "                patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "                output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "                gt_index = n\n",
    "                ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "                score = np.sum(output_class*ground_truth)\n",
    "                batch_score_list.append(score)\n",
    "                \n",
    "        yield batch_score_list\n",
    "\n",
    "g = compute_accuracies()\n",
    "score_list = []\n",
    "\n",
    "for _ in range(6): score_list.append(next(g))\n",
    "\n",
    "accuracies = [np.mean(scores)*100 for scores in score_list]\n",
    "#print(f'{j} embeddings did not pass the threshold')\n",
    "#return accuracies\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6871dd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.figure(figsize=(12,7), dpi=300)\n",
    "plt.bar(neurotransmitters, accuracies)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Mean hard accuracy')\n",
    "#plt.title(f'Mean hard accuracies across classes - {model_size} DINOv2 - 140x140 images - Threshold = {distance_threshold} - Data augmentation: {data_aug}')\n",
    "plt.axhline(np.mean(accuracies), color='r', linestyle='--', label='Average')\n",
    "plt.axhline(y=(100/6), color='b', linestyle='--', label='Randomness')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,110])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524483d",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7510ba",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.Neuro_Classification.Neuro_Classification_Head import head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e494ff",
   "metadata": {},
   "source": [
    "# Plot MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(head, input_size=(1,feat_dim), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515c93a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.Neuro_Classification.Neuro_Classification_Head_training import head_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "loss_list, proportion_list, prediction_list, test_accuracies = head_training(nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3785c58",
   "metadata": {},
   "source": [
    "# Class proportions during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa85bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import class_proprtions\n",
    "\n",
    "class_proprtions(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225436ad",
   "metadata": {},
   "source": [
    "# Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import confusion_matrix\n",
    "\n",
    "split = '80/20'\n",
    "confusion_matrix(prediction_list, nb_epochs, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f4da8",
   "metadata": {},
   "source": [
    "# Progression curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import training_curve\n",
    "\n",
    "training_curve(epochs, loss_list, test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b836307",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219d0d2",
   "metadata": {},
   "source": [
    "# -UMAP Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae376e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_file = get_fnames()[1][0]\n",
    "ex_image = resize_hdf_image(load_image(ex_file)[0])[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([ex_image, ex_image, ex_image], axis=3).transpose(0,3,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30714c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_augmented_few_shot = DinoPsd_pipeline(augmented_model[0],\n",
    "                                      augmented_model[0].patch_size,\n",
    "                                      device,\n",
    "                                      get_img_processing_f(resize_size),\n",
    "                                      feat_dim, \n",
    "                                      dino_image_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b584bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(list(EX_EMBEDDING.values())[0]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb280e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = untrained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "untrained_augmented_few_shot.pre_compute_embeddings(\n",
    "    ex_image,\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "EX_EMBEDDING = augmented_model[0].forward_features(torch.from_numpy(np.concatenate([ex_image, ex_image, ex_image], axis=3).transpose(0,3,1,2)).to(torch.float32).to(device))#\n",
    "\n",
    "ex = untrained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "neuro = 1\n",
    "EX_REF = torch.tensor(REFS[neuro]).cpu()\n",
    "\n",
    "embeddings_and_ref = np.vstack([EX_REF, EX_EMBEDDING])\n",
    "\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "semantic_distances = compute_similarity_matrix(EX_REF, EX_EMBEDDING)\n",
    "\n",
    "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(20,10), dpi=100)\n",
    "\n",
    "sc = ax1.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "ax1.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='lime', marker='o', label='Reference embedding')\n",
    "cbar1 = plt.colorbar(sc, ax=ax1)\n",
    "cbar1.set_label('Semantic distance')\n",
    "ax1.legend()\n",
    "ax1.set_title(f'Before training - Semantic distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "sc2 = ax2.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=distances, s=2, cmap='Greens_r')\n",
    "ax2.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='lime', marker='o', label='Mean acc reference embedding')\n",
    "cbar2 = plt.colorbar(sc2, ax=ax2)\n",
    "cbar2.set_label('Spatial distance')\n",
    "ax2.legend()\n",
    "ax2.set_title(f'Before training - Spatial distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd87a66",
   "metadata": {},
   "source": [
    "# -Training augmented model with frozen MLP Head (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac75c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "454a44dd",
   "metadata": {},
   "source": [
    "# -UMAP After:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004ed27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_augmented_few_shot = DinoSim_pipeline(augmented_model[0],\n",
    "                                      augmented_model[0].patch_size,\n",
    "                                      device,\n",
    "                                      get_img_processing_f(resize_size),\n",
    "                                      feat_dim, \n",
    "                                      dino_image_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "trained_augmented_few_shot.pre_compute_embeddings(\n",
    "    ex_image,\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "NEW_EX_EMBEDDING = trained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "EX_REF = torch.tensor(REFS[0]).cpu()\n",
    "\n",
    "new_embeddings_and_ref = np.vstack([EX_REF, NEW_EX_EMBEDDING])\n",
    "\n",
    "\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "\n",
    "\n",
    "new_umap_embeddings = reducer.fit_transform(new_embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "new_semantic_distances = compute_similarity_matrix(EX_REF, NEW_EX_EMBEDDING)\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(20,10), dpi=100)\n",
    "\n",
    "sc = ax1.scatter(new_umap_embeddings[1:,0], new_umap_embeddings[1:,1], c=new_semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "ax1.scatter(new_umap_embeddings[0,0], new_umap_embeddings[0,1], c='lime', marker='o', label='Reference embedding')\n",
    "cbar1 = plt.colorbar(sc, ax=ax1)\n",
    "cbar1.set_label('Semantic distance')\n",
    "ax1.legend()\n",
    "ax1.set_title(f'After training - Semantic distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "sc2 = ax2.scatter(new_umap_embeddings[1:,0], new_umap_embeddings[1:,1], c=distances, s=2, cmap='Greens_r')\n",
    "ax2.scatter(new_umap_embeddings[0,0], new_umap_embeddings[0,1], c='lime', marker='o', label='Mean acc reference embedding')\n",
    "cbar2 = plt.colorbar(sc2, ax=ax2)\n",
    "cbar2.set_label('Spatial distance')\n",
    "ax2.legend()\n",
    "ax2.set_title(f'After training - Spatial distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f7061",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf77de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading embeddings\n",
      "Done loading reference embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4276) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f868fbc3cd47afb48ddcdc1324a5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Comparing embeddings to reference:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 4928400 and the array at index 1 has size 3600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m REST \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(REST_list)\n\u001b[1;32m     38\u001b[0m PSDs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(PSD_list)\n\u001b[0;32m---> 40\u001b[0m DATA \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mREST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPSDs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m REST_LABELS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m, REST\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     44\u001b[0m REST_LABELS[\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 4928400 and the array at index 1 has size 3600"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from random import sample\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from DinoPsd import DinoPsd_pipeline\n",
    "from DinoPsd_utils import get_img_processing_f\n",
    "from Fine_Tuning.compute_embeddings import compute_ref_embeddings\n",
    "\n",
    "from setup import resize_size, embeddings_path, neurotransmitters, feat_dim\n",
    "\n",
    "\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS)\n",
    "\n",
    "print('Done loading embeddings')\n",
    "\n",
    "\n",
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "print('Done loading reference embeddings')\n",
    "\n",
    "PSD_list, REST_list = [], []\n",
    "\n",
    "for image in tqdm(EMBEDDINGS, desc='Comparing embeddings to reference'):\n",
    "    flattened_image = image.reshape(-1, feat_dim)\n",
    "    similarity_matrix = cosine_similarity(REFS, flattened_image)\n",
    "    best_index = np.unravel_index(np.argmax(similarity_matrix, axis=None), similarity_matrix.shape)[1]\n",
    "    PSD_list.append(flattened_image[best_index])\n",
    "    REST_list.append(torch.cat([flattened_image[:best_index], flattened_image[best_index:]]))\n",
    "\n",
    "\n",
    "REST = torch.vstack(REST_list)\n",
    "PSDs = torch.stack(PSD_list)\n",
    "\n",
    "DATA = np.concatenate([REST, PSDs], axis=0)\n",
    "\n",
    "\n",
    "REST_LABELS = np.zeros((2, REST.shape[0]))\n",
    "REST_LABELS[0,:] = 1\n",
    "\n",
    "PSD_LABELS = np.zeros((2,PSDs.shape[0]))\n",
    "PSD_LABELS[1,:] = 1\n",
    "\n",
    "LABELS = np.concatenate([REST_LABELS, PSD_LABELS], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfcc23a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DATA = np.concatenate([REST, PSDs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75bbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961911c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m neurotransmitters\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MVA/Internship/Cambridge/Code/Cambridge/DinoPsd_code/src/setup.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtifffile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m imread\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/__init__.py:2108\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   2106\u001b[0m \n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[0;32m-> 2108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/functional.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[1;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[1;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     CELU,\n\u001b[1;32m      5\u001b[0m     ELU,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     Threshold,\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[1;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/utils/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[1;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[1;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[1;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[1;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[1;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from random import sample\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from DinoPsd import DinoPsd_pipeline\n",
    "from DinoPsd_utils import get_img_processing_f\n",
    "from Fine_Tuning.compute_embeddings import compute_ref_embeddings\n",
    "\n",
    "from setup import resize_size, embeddings_path, neurotransmitters, feat_dim\n",
    "\n",
    "\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS)\n",
    "\n",
    "print('Done loading embeddings')\n",
    "\n",
    "\n",
    "REFS = torch.load(os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "PSD_list, REST_list = [], []\n",
    "\n",
    "for image in tqdm(EMBEDDINGS, desc='Comparing embeddings to reference'):\n",
    "    flattened_image = image.reshape(-1, feat_dim)\n",
    "    similarity_matrix = cosine_similarity(REFS, flattened_image)\n",
    "    best_index = np.unravel_index(np.argmax(similarity_matrix, axis=None), similarity_matrix.shape)[1]\n",
    "    PSD_list.append(flattened_image[best_index])\n",
    "    REST_list.append(torch.cat([flattened_image[:best_index], flattened_image[best_index:]]))\n",
    "\n",
    "\n",
    "REST = torch.stack(REST_list)\n",
    "PSDs = torch.vstack(PSD_list)\n",
    "\n",
    "DATA = np.concatenate([REST, PSDs], axis=1)\n",
    "\n",
    "\n",
    "REST_LABELS = np.zeros((2, REST.shape[0]))\n",
    "REST_LABELS[0,:] = 1\n",
    "\n",
    "PSD_LABELS = np.zeros((2,PSDs.shape[0]))\n",
    "PSD_LABELS[1,:] = 1\n",
    "\n",
    "LABELS = np.concatenate([REST_LABELS, PSD_LABELS], axis = 1)\n",
    "\n",
    "#===================================================================\n",
    "\n",
    "DATASET = list(zip(DATA, LABELS))\n",
    "\n",
    "DATASET = sample(DATASET, len(DATASET))\n",
    "\n",
    "\n",
    "test_proportion = 0.2\n",
    "\n",
    "\n",
    "SPLIT = int(len(DATASET)*test_proportion)\n",
    "TRAINING_SET = DATASET[SPLIT:]\n",
    "TEST_SET = DATASET[:SPLIT]\n",
    "\n",
    "one_hot_neurotransmitters = np.eye(len(neurotransmitters))\n",
    "\n",
    "\n",
    "class Custom_LP_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 set):\n",
    "        if set == 'training':\n",
    "            self.data = TRAINING_SET\n",
    "        else:\n",
    "            self.data = TEST_SET\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding, label = self.data[idx]\n",
    "        label_idx = neurotransmitters.index(label[0])\n",
    "        return embedding, one_hot_neurotransmitters[label_idx]\n",
    "    \n",
    "    \n",
    "train_batch_size, test_batch_size = 50, 50\n",
    "\n",
    "training_dataset = Custom_LP_Dataset('training') \n",
    "test_dataset = Custom_LP_Dataset('test')\n",
    "\n",
    "training_loader = utils.DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
