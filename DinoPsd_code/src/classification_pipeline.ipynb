{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0924a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d004fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, curated_idx, few_shot_transforms,  embeddings_path, model\n",
    "from setup import tqdm, torch, np, os, plt, tqdm, gc, sample\n",
    "from analysis_utils import display_hdf_image_grid, resize_hdf_image, get_augmented_coordinates\n",
    "from setup import cosine_similarity, euclidean_distances\n",
    "from perso_utils import get_fnames, load_image, get_latents\n",
    "from DinoPsd import DinoPsd_pipeline\n",
    "from DinoPsd_utils import get_img_processing_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fba314",
   "metadata": {},
   "source": [
    "### Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = DinoPsd_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n",
    "\n",
    "files, labels = zip(*get_fnames()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_refs = compute_ref_embeddings(True, os.path.join(embeddings_path, 'giant_mean_ref_518_Aug=False_k=10'))\n",
    "#mean_refs = compute_ref_embeddings(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=True):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "            imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "            imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "            good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "            \n",
    "            good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "            good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "            good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "        unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "        for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "            \n",
    "            # Pre-compute embeddings\n",
    "            few_shot.pre_compute_embeddings(\n",
    "                dataset,  # Pass numpy array of images\n",
    "                overlap=(0.5, 0.5),\n",
    "                padding=(0, 0),\n",
    "                crop_shape=(518, 518, 1),\n",
    "                verbose=True,\n",
    "                batch_size=10\n",
    "            )\n",
    "            \n",
    "            # Set reference vectors\n",
    "            few_shot.set_reference_vector(coordinates, filter=None)\n",
    "            ref = few_shot.get_refs()\n",
    "            \n",
    "            # Get closest elements - using the correct method name\n",
    "            close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "            k_labels =  [batch_label for _ in range(k)]\n",
    "\n",
    "            \n",
    "            # Convert to numpy for storing\n",
    "            close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "            \n",
    "            filtered_latent_list.append(close_embedding_np)\n",
    "            filtered_label_list.append(k_labels)\n",
    "            \n",
    "            # Clean up to free memory\n",
    "            few_shot.delete_precomputed_embeddings()\n",
    "            few_shot.delete_references()\n",
    "\n",
    "        mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\n",
    "        # Stack all embeddings and labels\n",
    "        ref_latents = np.vstack(filtered_latent_list)\n",
    "        ref_labels = np.hstack(filtered_label_list)\n",
    "        \n",
    "        torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "        torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7274",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_neurotransmitters = np.eye(len(neurotransmitters))\n",
    "#emb_labels = np.hstack([[neuro]*int((resize_size/14)**2 * 600) for neuro in neurotransmitters]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859a151",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Datasetwide Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ecbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_embeddings = compute_embeddings()\n",
    "#new_embeddings = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt')) # takes ~ 45 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef9b0c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute class-wise accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39697",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from analysis_utils import get_threshold\n",
    "\n",
    "def compute_accuracies(reference_embeddings = mean_refs, \n",
    "                       embeddings = new_embeddings,\n",
    "                       metric = euclidean_distances,\n",
    "                       distance_threshold = 0.01\n",
    "                       ):\n",
    "\n",
    "    batch_size = int(len(embeddings)/6)\n",
    "\n",
    "    for n, i in tqdm(enumerate(range(0, len(embeddings), batch_size))):\n",
    "        batch = embeddings[i:i+batch_size]\n",
    "\n",
    "        #embeddings = embeddings.reshape(-1, feat_dim)\n",
    "        similarity_matrix = metric(reference_embeddings, batch)\n",
    "        similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "        threshold = get_threshold(similarity_matrix_normalized, 0.9)\n",
    "        similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= threshold, similarity_matrix_normalized, 0)\n",
    "\n",
    "        batch_score_list = []\n",
    "        for k in range(batch_size):\n",
    "\n",
    "            column = similarity_matrix_normalized_filtered[:,k]\n",
    "            j=0\n",
    "            if sum(column) == 0:\n",
    "                j+=1\n",
    "            else:\n",
    "                patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "                output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "                gt_index = n\n",
    "                ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "                score = np.sum(output_class*ground_truth)\n",
    "                batch_score_list.append(score)\n",
    "                \n",
    "        yield batch_score_list\n",
    "\n",
    "g = compute_accuracies()\n",
    "score_list = []\n",
    "\n",
    "for _ in range(6): score_list.append(next(g))\n",
    "\n",
    "accuracies = [np.mean(scores)*100 for scores in score_list]\n",
    "#print(f'{j} embeddings did not pass the threshold')\n",
    "#return accuracies\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6871dd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.figure(figsize=(12,7), dpi=300)\n",
    "plt.bar(neurotransmitters, accuracies)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Mean hard accuracy')\n",
    "#plt.title(f'Mean hard accuracies across classes - {model_size} DINOv2 - 140x140 images - Threshold = {distance_threshold} - Data augmentation: {data_aug}')\n",
    "plt.axhline(np.mean(accuracies), color='r', linestyle='--', label='Average')\n",
    "plt.axhline(y=(100/6), color='b', linestyle='--', label='Randomness')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,110])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524483d",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7510ba",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.Neuro_Classification.Neuro_Classification_Head import head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e494ff",
   "metadata": {},
   "source": [
    "# Plot MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(head, input_size=(1,feat_dim), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515c93a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.Neuro_Classification.Neuro_Classification_Head_training import head_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "loss_list, proportion_list, prediction_list, test_accuracies = head_training(nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3785c58",
   "metadata": {},
   "source": [
    "# Class proportions during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa85bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import class_proprtions\n",
    "\n",
    "class_proprtions(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225436ad",
   "metadata": {},
   "source": [
    "# Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import confusion_matrix\n",
    "\n",
    "split = '80/20'\n",
    "confusion_matrix(prediction_list, nb_epochs, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f4da8",
   "metadata": {},
   "source": [
    "# Progression curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import training_curve\n",
    "\n",
    "training_curve(epochs, loss_list, test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b836307",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219d0d2",
   "metadata": {},
   "source": [
    "# -UMAP Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae376e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_file = get_fnames()[1][0]\n",
    "ex_image = resize_hdf_image(load_image(ex_file)[0])[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([ex_image, ex_image, ex_image], axis=3).transpose(0,3,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30714c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_augmented_few_shot = DinoPsd_pipeline(augmented_model[0],\n",
    "                                      augmented_model[0].patch_size,\n",
    "                                      device,\n",
    "                                      get_img_processing_f(resize_size),\n",
    "                                      feat_dim, \n",
    "                                      dino_image_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b584bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(list(EX_EMBEDDING.values())[0]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb280e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = untrained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "untrained_augmented_few_shot.pre_compute_embeddings(\n",
    "    ex_image,\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "EX_EMBEDDING = augmented_model[0].forward_features(torch.from_numpy(np.concatenate([ex_image, ex_image, ex_image], axis=3).transpose(0,3,1,2)).to(torch.float32).to(device))#\n",
    "\n",
    "ex = untrained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "neuro = 1\n",
    "EX_REF = torch.tensor(REFS[neuro]).cpu()\n",
    "\n",
    "embeddings_and_ref = np.vstack([EX_REF, EX_EMBEDDING])\n",
    "\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "semantic_distances = compute_similarity_matrix(EX_REF, EX_EMBEDDING)\n",
    "\n",
    "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(20,10), dpi=100)\n",
    "\n",
    "sc = ax1.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "ax1.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='lime', marker='o', label='Reference embedding')\n",
    "cbar1 = plt.colorbar(sc, ax=ax1)\n",
    "cbar1.set_label('Semantic distance')\n",
    "ax1.legend()\n",
    "ax1.set_title(f'Before training - Semantic distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "sc2 = ax2.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=distances, s=2, cmap='Greens_r')\n",
    "ax2.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='lime', marker='o', label='Mean acc reference embedding')\n",
    "cbar2 = plt.colorbar(sc2, ax=ax2)\n",
    "cbar2.set_label('Spatial distance')\n",
    "ax2.legend()\n",
    "ax2.set_title(f'Before training - Spatial distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd87a66",
   "metadata": {},
   "source": [
    "# -Training augmented model with frozen MLP Head (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac75c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "454a44dd",
   "metadata": {},
   "source": [
    "# -UMAP After:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004ed27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_augmented_few_shot = DinoSim_pipeline(augmented_model[0],\n",
    "                                      augmented_model[0].patch_size,\n",
    "                                      device,\n",
    "                                      get_img_processing_f(resize_size),\n",
    "                                      feat_dim, \n",
    "                                      dino_image_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "trained_augmented_few_shot.pre_compute_embeddings(\n",
    "    ex_image,\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "NEW_EX_EMBEDDING = trained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "EX_REF = torch.tensor(REFS[0]).cpu()\n",
    "\n",
    "new_embeddings_and_ref = np.vstack([EX_REF, NEW_EX_EMBEDDING])\n",
    "\n",
    "\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "\n",
    "\n",
    "new_umap_embeddings = reducer.fit_transform(new_embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "new_semantic_distances = compute_similarity_matrix(EX_REF, NEW_EX_EMBEDDING)\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(20,10), dpi=100)\n",
    "\n",
    "sc = ax1.scatter(new_umap_embeddings[1:,0], new_umap_embeddings[1:,1], c=new_semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "ax1.scatter(new_umap_embeddings[0,0], new_umap_embeddings[0,1], c='lime', marker='o', label='Reference embedding')\n",
    "cbar1 = plt.colorbar(sc, ax=ax1)\n",
    "cbar1.set_label('Semantic distance')\n",
    "ax1.legend()\n",
    "ax1.set_title(f'After training - Semantic distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "sc2 = ax2.scatter(new_umap_embeddings[1:,0], new_umap_embeddings[1:,1], c=distances, s=2, cmap='Greens_r')\n",
    "ax2.scatter(new_umap_embeddings[0,0], new_umap_embeddings[0,1], c='lime', marker='o', label='Mean acc reference embedding')\n",
    "cbar2 = plt.colorbar(sc2, ax=ax2)\n",
    "cbar2.set_label('Spatial distance')\n",
    "ax2.legend()\n",
    "ax2.set_title(f'After training - Spatial distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f7061",
   "metadata": {},
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = list(range(1, nb_iterations+1))\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5), dpi=150)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Training\n",
    "lns1 = ax1.plot(x, general_loss_list[0], label='DINOv2 Train Loss', color='blue')\n",
    "\n",
    "lns2 = ax1.plot(x, general_loss_list[1], label='MLP Train Loss', color='cyan')\n",
    "ax1.set_ylim(0, max(max(general_loss_list)) * 1.05)\n",
    "ax1.set_ylabel('Train Loss')\n",
    "\n",
    "#Test\n",
    "lns3 = ax2.plot(x, general_accuracy_list[0], label='DINOv2 Test Accuracy', color='green')\n",
    "\n",
    "\n",
    "lns4 = ax2.plot(x, general_accuracy_list[1], label='MLP Test Accuracy', color='lime')\n",
    "ax2.set_ylim(0, 100 * 1.05)\n",
    "ax2.set_ylabel('Test Accuracy')\n",
    "\n",
    "\n",
    "# Combine legends\n",
    "lns = lns1 + lns2 + lns3 + lns4\n",
    "labels = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labels, loc=6)\n",
    "\n",
    "plt.locator_params(axis='x', nbins=2)\n",
    "\n",
    "ax1.set_xlabel('Iterations')\n",
    "plt.title('Training Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d466672",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85355df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/tomw/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n",
      "Done loading embeddings\n",
      "Computing UMAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import umap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from setup import embeddings_path, feat_dim\n",
    "\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "print('Loading embeddings')\n",
    "\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))[:600]\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS).reshape(-1, feat_dim)\n",
    "\n",
    "print('Done loading embeddings')\n",
    "print('Computing UMAP')\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(EMBEDDINGS)\n",
    "\n",
    "plt.plot(umap_embeddings[:,0], umap_embeddings[:,1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce674d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random \n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from DinoPsd import DinoPsd_pipeline\n",
    "from DinoPsd_utils import get_img_processing_f\n",
    "\n",
    "from setup import embeddings_path, feat_dim\n",
    "\n",
    "\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS)\n",
    "\n",
    "print('Done loading embeddings')\n",
    "\n",
    "\n",
    "REFS = torch.load(os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'), weights_only=False)\n",
    "\n",
    "print('Done loading reference embeddings')\n",
    "\n",
    "\n",
    "PSD_list, REST_list = [], []\n",
    "\n",
    "nb_best_patches = 10\n",
    "\n",
    "for image in tqdm(EMBEDDINGS, desc='Comparing embeddings to reference'):\n",
    "    flattened_image = image.reshape(-1, feat_dim)\n",
    "    similarity_matrix = cosine_similarity(REFS, flattened_image)\n",
    "    best_indices = np.unravel_index(similarity_matrix.ravel().argsort()[-nb_best_patches:][::-1], similarity_matrix.shape)[1]\n",
    "    PSD_list.extend(flattened_image[best_indices])\n",
    "    REST_list.extend(np.delete(flattened_image, best_indices, axis=0))\n",
    "\n",
    "\n",
    "REST_list = random.choices(REST_list, k=len(PSD_list))\n",
    "\n",
    "REST = torch.stack(REST_list)\n",
    "PSD = torch.stack(PSD_list)\n",
    "\n",
    "DATA = np.concatenate([REST, PSD], axis=0)\n",
    "\n",
    "#assert int(REST.shape[0]) == int(PSD.shape[0]), 'Unbalanced dataset'\n",
    "\n",
    "\n",
    "REST_LABELS = np.zeros((REST.shape[0], 2))\n",
    "REST_LABELS[:,0] = 1\n",
    "\n",
    "PSD_LABELS = np.zeros((PSD.shape[0], 2))\n",
    "PSD_LABELS[:,1] = 1\n",
    "\n",
    "LABELS = np.concatenate([REST_LABELS, PSD_LABELS], axis = 0)\n",
    "\n",
    "#assert int(LABELS.shape[0]) == int(DATA.shape[0]), 'Problem with labelization procedure'\n",
    "\n",
    "\n",
    "DATASET = list(zip(DATA, LABELS))\n",
    "\n",
    "DATASET = random.sample(DATASET, len(DATASET))\n",
    "\n",
    "\n",
    "test_proportion = 0.2\n",
    "\n",
    "\n",
    "SPLIT = int(len(DATASET)*test_proportion)\n",
    "TRAINING_SET = DATASET[SPLIT:]\n",
    "TEST_SET = DATASET[:SPLIT]\n",
    "\n",
    "\n",
    "class Custom_Detection_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 set):\n",
    "        if set == 'training':\n",
    "            self.data = TRAINING_SET\n",
    "        else:\n",
    "            self.data = TEST_SET\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "\n",
    "train_batch_size, test_batch_size = 50, 50\n",
    "\n",
    "training_dataset = Custom_Detection_Dataset('training') \n",
    "test_dataset = Custom_Detection_Dataset('test')\n",
    "\n",
    "training_loader = utils.DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb44469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading embeddings\n",
      "Done loading reference embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f25707de5647f387870eb856933ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Comparing embeddings to reference:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random \n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from DinoPsd import DinoPsd_pipeline\n",
    "from DinoPsd_utils import get_img_processing_f\n",
    "\n",
    "from setup import embeddings_path, feat_dim\n",
    "\n",
    "_EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "_EMBEDDINGS = np.stack(_EMBEDDINGS).reshape(-1, feat_dim)\n",
    "\n",
    "nb_columns = _EMBEDDINGS.shape[0]\n",
    "columns = np.random.choice(nb_columns, size=nb_columns, replace=False)\n",
    "\n",
    "EMBEDDINGS = _EMBEDDINGS[columns, :]\n",
    "\n",
    "assert EMBEDDINGS.shape == _EMBEDDINGS.shape\n",
    "\n",
    "print('Done loading embeddings')\n",
    "\n",
    "\n",
    "REFS = torch.load(os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'), weights_only=False)\n",
    "\n",
    "print('Done loading reference embeddings')\n",
    "\n",
    "\n",
    "PSD_list, REST_list = [], []\n",
    "\n",
    "nb_best_patches = 10\n",
    "\n",
    "for image in tqdm(EMBEDDINGS, desc='Comparing embeddings to reference'):\n",
    "    flattened_image = image.reshape(-1, feat_dim)\n",
    "    similarity_matrix = cosine_similarity(REFS, flattened_image)\n",
    "    flat_similarities = np.unique(similarity_matrix.ravel())\n",
    "    top_10_flat_indices = flat_similarities.argsort()[-nb_best_patches:][::-1]\n",
    "    best_indices = np.unravel_index(top_10_flat_indices, similarity_matrix.shape)[1]\n",
    "    PSD_list.extend(flattened_image[best_indices])\n",
    "    REST_list.extend(np.delete(flattened_image, best_indices, axis=0))\n",
    "\n",
    "\n",
    "PSD = torch.stack(PSD_list)\n",
    "\n",
    "REST = torch.stack(REST_list)\n",
    "#REST_SUBLISTS = [REST[i:i+len(PSD_list)] for i in range(0, len(REST), len(PSD_list))]\n",
    "\n",
    "\n",
    "PSD_LABELS = np.zeros((PSD.shape[0], 2))\n",
    "PSD_LABELS[:,1] = 1\n",
    "\n",
    "REST_LABELS = np.zeros((REST.shape[0], 2))\n",
    "REST_LABELS[:,0] = 1\n",
    "\n",
    "\n",
    "LABELLED_PSD = list(zip(PSD, PSD_LABELS))\n",
    "\n",
    "LABELLED_REST = list(zip(REST, REST_LABELS))\n",
    "\n",
    "\n",
    "class Custom_Detection_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 set,\n",
    "                 test_proportion):\n",
    "        \n",
    "        self.test_proportion = test_proportion\n",
    "\n",
    "        self.psd = LABELLED_PSD \n",
    "        \n",
    "        if set == 'training':\n",
    "            self.data = TRAINING_SET\n",
    "        else:\n",
    "            self.data = TEST_SET\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        SPLIT = int(len(DATASET)*self.test_proportion)\n",
    "        TRAINING_SET = DATASET[SPLIT:]\n",
    "        TEST_SET = DATASET[:SPLIT]\n",
    "\n",
    "        psds = \n",
    "        \n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "\n",
    "train_batch_size, test_batch_size = 50, 50\n",
    "\n",
    "training_dataset = Custom_Detection_Dataset('training') \n",
    "test_dataset = Custom_Detection_Dataset('test')\n",
    "\n",
    "training_loader = utils.DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91321a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/tomwelch/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
